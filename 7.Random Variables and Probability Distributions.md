# Random Variables and Probability Distributions

## Random Variables
- **Definition**: A numerical variable whose value **depends on the outcome** of a chance experiment.(use capital letters to denote)
- **Discrete Random Variable**: Takes values that are **isolated** points on the number line (e.g., number of heads in coin flips).
- **Continuous Random Variable**: Takes values that include an **entire interval** on the number line (e.g., time spent in a store).
## Discrete Random Variables
### Probability Distributions
- **Probability Mass Function (PMF)**: $p_X(x) = P(X = x)$ for all $x$.
  - **Properties**:
    - For every possible $x$ value a random variable $X$ could
take,  $0 \leq p_X(x) \leq 1$.
    - $\sum_x p_X(x) = 1$
- **Cumulative Distribution Function (CDF)**: $F_X(x) = P(X \leq x)$ for all $x$.
- **Transformations between pmf and cdf**:
  - **pmf $\rightarrow$ cdf** : $F_X(x) = \sum_{y \leq x} p_X(y)$.
  - **cdf $\rightarrow$ pmf** : Suppose $X$ takes ordered values $x_{1}, x_{2}, x_{3}, \cdots $, then $p_X(x_i) = P(X = x_i) = P(x_{i-1} < X \leq x_i) = P(X \leq x_i) - P(X \leq x_{i-1}) = F(x_i) - F(x_{i-1})$


### Multiple Discrete Random Variables
- **Joint Probability Mass Function (PMF)**:  Describe the probability distribution of two or more random variables together. $p_{X,Y}(x, y) := P(X = x \text{ and } Y = y) $.
- **Marginal Probability Mass Function**: Describe the probability distribution of a single random variable when the joint distribution is known.
  - $p_X(x) = \sum_y p_{X,Y}(x, y)$
  - $p_Y(y) = \sum_x p_{X,Y}(x, y)$
    
  **Note**: For the marginal of $X$, we sum over all values of $y$. For the marginal of $Y$, we sum over all values of $x$
- **Independence between X and Y (Discrete)** :Two discrete random variables $X$ and $Y$ are independent if $p_{X,Y}(x, y) = p_X(x) \cdot p_Y(y)$ for all $x$ and $y$. ( $p_{X|Y}(x|y) = p_X(x)$ )
  -  If $X$ and $Y$ are independent, then $E[XY] = E[X]E[Y]$ and $Var(X + Y) = Var(X) +Var(Y)$.

### Expectation and Variance
- **Expectation (E[X])**: The **long-term average value** of repetitions of the experiment it represents. (mean value or expected value, and denotation $\mu_{x}$ could be used alternatively). $E[X] = \sum_x x \cdot p_X(x)$
  - **Linearity of Expectation**: $E[aX + b] = aE[X] + b$
    
    (Multiple Random Variables): $E[aX + bY + c] = aE[X] + bE[Y] + c$
- **Variance (Var(X))**: $Var(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2 \geq 0$
  - $Var(aX + b) = a^2Var(X)$
- **Standard Deviation (ÏƒX)**: $\sigma_{X} = \sqrt{Var(X)}$


## Continuous Random Variables
### Probability Distributions
- **Probability Density Function (PDF)**: For a continuous random variable $X$, the Probability Density Function (PDF) of $X$ is $f(x)$ where $P(X = x) = 0$ for all $x$ and for any $a \leq b$ :
  
<div align="center">
  
  $P(a < X < b) = \int_a^b f(x)dx$
</div>
  
- **Cumulative Distribution Function (CDF)**: $F_{X}(x)= P(X \leq x) = \int_{-\infty}^x f_{X}(y)dy$.
- **Transformation between pdf and cdf**:
  - **pdf $\rightarrow$ cdf**: $F_{X}(x) = \int_{-\infty}^x f_{X}(y)dy$.
  - **cdf $\rightarrow$ pdf**: $f_{X}(x) = \frac{d}{dx}F_{X}(x)$

### Multiple Continuous Random Variables
- **Joint Probability Density Function (PMF)**:  An integrable function f(x, y) such that
  - $f(x,y) \geq 0$
  - $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x,y)dxdy = 1 $
  - $P((X,Y)\in A) = \int\int_{A} f(x,y)dxdy$
- **Marginal Probability Density Function**:
  - $f_X(x) = \int_{-\infty}^{\infty} f(x, y)dy$
  - $f_Y(y) = \int_{-\infty}^{\infty} f(x, y)dx$
- **Independence between X and Y (Continuous)**: Two jointly continuous random variables $X$ and $Y$ are independent if $f_{X,Y}(x, y) = f_X(x) \cdot f_Y(y)$ for all $x$ and $y$.
  
### Expectation and Variance
- **Expectation**: $E[X] = \int_{-\infty}^{\infty} xf(x)dx$
- **Variance**: $\text{Var}(X) = \int_{-\infty}^{\infty} (x - E[X])^2 f(x)dx = \int_{-\infty}^{\infty} x^2f(x)dx - E^2[X]$

## Covariance
- **Covariance**: $\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]$.
  - If $\text{Cov}(X, Y) = 0$, $X$ and $Y$ are uncorrelated.
    
  If X and Y are **independent** and if their covariance exists, then they are
**uncorrelated**. The **converse is not necessarily true**
- **Properties of Covariance**:
  - $\text{Cov}(X, Y) =\text{Cov}(Y, X)$
  - if c is a constant, $\text{Cov}(X, c) = 0$
  - Linearity: $\text{Cov}(aX+b, Y) = \text{Cov}(aX, Y) + \text{Cov}(b, Y) = a\text{Cov}(X, Y)$
  - More general, Covariance is Bilinear: $\text{Cov}(aX + bY, cU + dV) = ac\text{Cov}(X, U) + ad\text{Cov}(X,V)+ bc\text{Cov}(Y, U) + bd\text{Cov}(Y,V)$
- **Correlation Coefficient**: $\rho_{X,Y} = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X) \text{Var}(Y)}}$.
- **General Formula Variance of Sum of Random Variables**
  - Sum of two:  $\text{Var}(X_1 + X_2) = \text{Var}(X_1) + \text{Var}(X_2) + 2\text{Cov}(X_1, X_2)$
  - Sum of three: $\text{Var}(X_1 + X_2 + X_3) = \text{Var}(X_1) + \text{Var}(X_2) + \text{Var}(X_3) + 2\text{Cov}(X_1, X_2) + 2\text{Cov}(X_1, X_3) + 2\text{Cov}(X_2, X_3)$

