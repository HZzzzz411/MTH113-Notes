# Random Variables and Probability Distributions

## Random Variables
- **Definition**: A numerical variable whose value **depends on the outcome** of a chance experiment.(use capital letters to denote)
- **Discrete Random Variable**: Takes values that are **isolated** points on the number line (e.g., number of heads in coin flips).
- **Continuous Random Variable**: Takes values that include an **entire interval** on the number line (e.g., time spent in a store).

## Probability Distributions
- **Probability Mass Function (PMF)**: $p_X(x) = P(X = x)$ for all $x$.
  - **Properties**:
    - For every possible $x$ value a random variable $X$ could
take,  $0 \leq p_X(x) \leq 1$.
    - $\sum_x p_X(x) = 1$
- **Cumulative Distribution Function (CDF)**: $F_X(x) = P(X \leq x)$ for all $x$.
- **Transformations between pmf and cdf**:
  - **pmf $\rightarrow$ cdf** : $F_X(x) = \sum_{y \leq x} p_X(y)$.
  - **cdf $\rightarrow$ pmf** : Suppose $X$ takes ordered values $x_{1}, x_{2}, x_{3}, \cdots $, then $p_X(x_i) = P(X = x_i) = P(x_{i-1} < X \leq x_i) = P(X \leq x_i) - P(X \leq x_{i-1}) = F(x_i) - F(x_{i-1})$

## Expectation and Variance
- **Expectation (E[X])**: The **long-term average value** of repetitions of the experiment it represents. (mean value or expected value, and denotation $\mu_{x}$ could be used alternatively). $E[X] = \sum_x x \cdot p_X(x)$
  - **Linearity of Expectation**: $E[aX + b] = aE[X] + b$
- **Variance (Var(X))**: $Var(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2 \geq 0$
- **Standard Deviation (σX)**: $\sigma_{X} = \sqrt{Var(X)}$

## Properties of Expectation and Variance
- **Linearity of Expectation**: $E[aX + b] = aE[X] + b$.
- **Variance of Linear Functions**: \( Var(aX + b) = a^2Var(X) \).

## Multiple Random Variables
- **Joint Distribution Functions**: Describe the probability distribution of two or more random variables together.
- **Marginal Distribution Functions**: Describe the probability distribution of a single random variable when the joint distribution is known.
- **Independence**: Two random variables X and Y are independent if \( P(X \cap Y) = P(X)P(Y) \).
- **Covariance and Correlation**: Measure the linear relationship between two random variables.
  - **Covariance**: \( \text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])] \).
  - **Correlation**: \( \rho_{X,Y} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y} \).

## Key Terms and Concepts
- Random variable
- Discrete random variable
- Continuous random variable
- Probability mass function (PMF)
- Cumulative distribution function (CDF)
- Expectation (E[X])
- Variance (Var(X))
- Standard deviation (σX)
- Linearity of expectation
- Variance of linear functions
- Joint distribution functions
- Marginal distribution functions
- Independence
- Covariance
- Correlation
