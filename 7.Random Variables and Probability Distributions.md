# Random Variables and Probability Distributions

## Random Variables
- **Definition**: A numerical variable whose value **depends on the outcome** of a chance experiment.(use capital letters to denote)
- **Discrete Random Variable**: Takes values that are **isolated** points on the number line (e.g., number of heads in coin flips).
- **Continuous Random Variable**: Takes values that include an **entire interval** on the number line (e.g., time spent in a store).

## Probability Distributions
- **Probability Mass Function (PMF)**: $p_X(x) = P(X = x)$ for all $x$.
  - **Properties**:
    - For every possible $x$ value a random variable $X$ could
take,  $0 \leq p_X(x) \leq 1$.
    - $\sum_x p_X(x) = 1$
- **Cumulative Distribution Function (CDF)**: $F_X(x) = P(X \leq x)$ for all $x$.
- **Transformations between pmf and cdf**:
  - **pmf $\rightarrow$ cdf** : $F_X(x) = \sum_{y \leq x} p_X(y)$.
  - **cdf $\rightarrow$ pmf** : Suppose $X$ takes ordered values $x_{1}, x_{2}, x_{3}, \cdots $, then $p_X(x_i) = P(X = x_i) = P(x_{i-1} < X \leq x_i) = P(X \leq x_i) - P(X \leq x_{i-1}) = F(x_i) - F(x_{i-1})$

## Expectation and Variance
- **Expectation (E[X])**: The **long-term average value** of repetitions of the experiment it represents. (mean value or expected value, and denotation $\mu_{x}$ could be used alternatively). $E[X] = \sum_x x \cdot p_X(x)$
  - **Linearity of Expectation**: $E[aX + b] = aE[X] + b$
- **Variance (Var(X))**: $Var(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2 \geq 0$
  - $Var(aX + b) = a^2Var(X)$
- **Standard Deviation (ÏƒX)**: $\sigma_{X} = \sqrt{Var(X)}$

## Multiple Random Variables
- **Joint Distribution Functions**: Describe the probability distribution of two or more random variables together.
- **Marginal Distribution Functions**: Describe the probability distribution of a single random variable when the joint distribution is known.
- **Independence**: Two random variables X and Y are independent if \( P(X \cap Y) = P(X)P(Y) \).
- **Covariance and Correlation**: Measure the linear relationship between two random variables.
  - **Covariance**: \( \text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])] \).
  - **Correlation**: \( \rho_{X,Y} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y} \).

## Discrete Random Variables
- **Joint Probability Mass Function (PMF)**: \( p_{X,Y}(x, y) = P(X = x \text{ and } Y = y) \).
- **Marginal Probability Mass Function**: 
  - \( p_X(x) = \sum_y p_{X,Y}(x, y) \)
  - \( p_Y(y) = \sum_x p_{X,Y}(x, y) \)
- **Expectation**: \( E[X] = \sum_x x \cdot p_X(x) \)
- **Variance**: \( \text{Var}(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2 \)

## Continuous Random Variables
- **Probability Density Function (PDF)**: \( f(x) \) such that \( P(a < X < b) = \int_a^b f(x) \, dx \).
- **Cumulative Distribution Function (CDF)**: \( F(x) = P(X \leq x) = \int_{-\infty}^x f(t) \, dt \).
- **Expectation**: \( E[X] = \int_{-\infty}^{\infty} x \cdot f(x) \, dx \)
- **Variance**: \( \text{Var}(X) = \int_{-\infty}^{\infty} (x - E[X])^2 f(x) \, dx \)

## Independence and Covariance
- **Independence**: Two random variables \( X \) and \( Y \) are independent if \( p_{X,Y}(x, y) = p_X(x) \cdot p_Y(y) \) for all \( x \) and \( y \).
- **Covariance**: \( \text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])] \).
  - If \( \text{Cov}(X, Y) = 0 \), \( X \) and \( Y \) are uncorrelated.
- **Correlation Coefficient**: \( \rho_{X,Y} = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X) \text{Var}(Y)}} \).

## Functions of Random Variables
- **Expectation of Functions**: 
  - For discrete: \( E[g(X)] = \sum_x g(x) \cdot p_X(x) \)
  - For continuous: \( E[g(X)] = \int_{-\infty}^{\infty} g(x) \cdot f(x) \, dx \)
- **Linearity of Expectation**: \( E[aX + bY + c] = aE[X] + bE[Y] + c \).

## Variance of Sums
- **Variance of Sum of Independent Random Variables**: 
  - \( \text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) \) if \( X \) and \( Y \) are independent.
- **General Formula**: 
  - \( \text{Var}(X_1 + X_2) = \text{Var}(X_1) + \text{Var}(X_2) + 2\text{Cov}(X_1, X_2) \).
