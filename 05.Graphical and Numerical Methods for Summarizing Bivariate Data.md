---
title: "Graphical and Numerical Methods for Summarizing Bivariate Data"
---
# Graphical and Numerical Methods for Summarizing Bivariate Data
## Bivariate Data
- **Bivariate Data**: Data consisting of **pairs of** numerical variables (x, y).
- **Scatter plot**: A graphical representation of bivariate data, showing the relationship between x and y.

## Relationships
- **Form**: Linear, non-linear.
- **Direction**: Positive (as x increases, y increases), Negative (as x increases, y decreases).
- **Strength**: Weak(-0.5 ~ 0.5), moderate($\pm$ 0.5 ~ 0.8), strong($\pm$ 0.8 ~ 1).

## Correlation
- **Correlation Coefficient (r)**: A numerical assessment of the **strength of the linear relationship** between x and y for a bivariate data set. $r \in [-1, 1]$
  
- **Sample Correlation Coefficient**:
 <div align="center">
   
$$ r = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2 \sum_{i=1}^{n} (y_i - \bar{y})^2}} = \frac{1}{n-1} \sum z_{x}z_{y}$$ 
 </div>
 
- **Properties of r**:
  - $r(x, y) = r(y, x)$ 
  - **Not affected** by **shifting** or **scaling**.
  - Only valid for linear relationships.
- **population correlation coefficient**:
<div align="center">
  <img src="https://github.com/user-attachments/assets/ed23b0b5-a1b7-425b-a598-3acf175fbb57" width=500 /">
</div>

## Linear Regression
- **Dependent Variable (y)**: The variable to be predicted (response variable).
- **Independent Variable (x)**: The variable used to predict y (explanatory
variable).
- **Least-Squares Regression Line**: The line that minimizes the sum of squared residuals.
  - Equation: $\hat{y} = a + bx$
    
  - Slope (b): $b = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2} = \frac{\sum xy-n\bar{x}\bar{y}}{\sum x^2 - n\bar{x}^2} = r \frac{s_{x}}{s_{y}}$
    
  - Intercept (a): $a = \bar{y} - b\bar{x}$
  - Sum of the Squared Residual **(SSR)**: $\sum_{i=1}^{n}(y_{i}-\hat y_{i})^2$
  - $\frac{\hat{y}-\bar{y}}{s_{y}} = r \frac{x-\bar{x}}{s_{x}}$
    
    If $x=\bar{x} \pm ks_{x}$, then $\hat{y}=\bar{y} \pm rks_{y}$. When |r| < 1, the predicted value of $y(\hat{y})$ is always closer to $\bar{y}$ than $x$ is to $\bar{x}$, in terms of standard deviations.

## Goodness-of-Fit
- **Residuals**: The **difference** between observed and predicted values. Residual = Observed Value - Predicted Value
- **Residual Plot**: A plot of residuals against the independent variable.
  - If linear regression was **appropriate** to summarize the relationship between x and y, then the **residual plot should have no pattern**.
    
  <div align="center">
  <img src="https://github.com/user-attachments/assets/56baa7af-a711-4388-9e20-72d8be77dd15" width=600 /">
</div>

- **Coefficient of Determination ($R^2$)**: The proportion of variability in y that can be explained by the linear relationship with x. (often use as a percentage)
  - $R^2 = (r)^2 = 1 -\frac{SSR}{SSTo}$
  - Sum of Squares Total (SSTo): $SSTo = \sum(y_{i} - \bar{y})$
  
- **Standard Deviation about the Regression Line ($s_{e}$)**: The standard deviation about the least-squares line is defined as
  <div align="center">
  $$s_{e} = \sqrt{\frac{SSR}{n-2}}=\sqrt{\frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{n - 2}}$$
  </div>
  

## Steps in Linear Regression Analysis
1. **Summarize the data graphically** by constructing a scatterplot.
2. **Decide if the relationship is approximately linear** based on the scatterplot.
3. **Find the equation of the least-squares line**.
4. **Construct a residual plot** and look for patterns or unusual features.(none, proceed to next step)
5. **Compute and interpret se and RÂ²**.
6. **Decide if the least-squares line is useful for making predictions**.
7. **Use the least-squares line to make predictions**.
